Lecture 5

对于逻辑回归或线性回归或广义线性模型：choose $\theta$ that maximize P(y|x)

对于generative learning algorithm: maximize P(x,y)

两种方法得出的决策边界略有不同

GDA使用不同的均值和相同的协方差矩阵，可以得出线性决策边界。可以使用不同的协方差矩阵，但会使参数增加，边界不再线性

如果x|y服从高斯分布，那GDA会更好用（GDA假设了高斯分布，相当于模型获得了更多信息）。但如果实际上不服从，那GDA就不太行。或者如果数据量比较小，可用GDA（加入更多假设信息）



朴素贝叶斯

一个特征向量（如某单词出现在邮件中则为1，没有则为0）

构建一个generative learning alg，算出有这些特征时为垃圾邮件的概率



lecture 6

当第一次收到某个单词时判断垃圾和非垃圾的概率都是0

解决方法：拉普拉斯平滑 如果有k种可能的事件，那在概率的分子上加1，在分母上加k

两种不同的特征表示法

multivariate bernoli model （定长。字典有多长特征向量就有多长，出现一次和出现多次没有区别）

multinomial event model （不定长，邮件多长特征向量就多长，出现一次就记录一次）

朴素贝叶斯的优点：简单，快，不用迭代 （quick and dirty）



support vector machine (SVM)

非线性决策边界，将数据映射到高维(x,y,x^2, y^2)

便于训练，有成熟的软件包，不太需要调参

几何间隔$\lambda$等于函数间隔$\hat{\lambda}$除以w的模

optimal margin classigier：choose w and b to maximize $\lambda$



lecuture 7

kernel trick

1.write algorithm in terms of $<x^{(i)},x^{(j)}>$ or <x,z>

2.let there be mapping from x->$\phi(x)$ 2D to high dimension or infinite dimension

3.find way to compute K(x,z)=$\phi(x)^T\phi(z)$

4.replace <x,z>in the algorithm with K(x,z)

run the whole learning algorithm on the high dimensional set of features $\phi$. 直接用超高维度的特征向量计算运算量很大



A的kernel是所有x使Ax=0 kernel和线代中的kernel区别？



lecture 8

标准训练流程：train sth quick and dirty, then find problems of high biases or high variance

bias：this learning algorithm has very strong preconception that the data could be fit by specific model

正则化是避免过拟合的主要方法之一

线性拟合优化项后加上一个$\frac{\lambda}{2}\left\|{\theta}\right\|$

lambda太小，过拟合，太大欠拟合